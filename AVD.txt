# app/main.py
import os
import json
import logging
import time
from typing import List, Optional, Dict, Any
from contextlib import contextmanager

from fastapi import FastAPI, HTTPException, BackgroundTasks, Depends
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()

# Database configuration
ENVIRONMENT = os.getenv("ENVIRONMENT", "local").lower()
MSSQL_CONFIG = {
    'environment': ENVIRONMENT,
    'local': {
        'driver': 'SQL Server', 
        'server': os.getenv("MSSQL_SERVER", "Mohamed"),
        'database': os.getenv("MSSQL_DATABASE", "gochat247"),
        'auth_type': 'windows',
        'library': 'pypyodbc'
    },
    'production': {
        'driver': 'ODBC Driver 17 for SQL Server',
        'server': os.getenv("MSSQL_PROD_SERVER", "AS2480DXBDB05"),
        'port': os.getenv("MSSQL_PROD_PORT", "1433"),
        'database': os.getenv("MSSQL_PROD_DATABASE", "gochat247"),
        'auth_type': 'kerberos',
        'library': 'pyodbc'
    },
    'staging': {
        'driver': 'ODBC Driver 17 for SQL Server',
        'server': os.getenv("MSSQL_STAGING_SERVER", "staging-server"),
        'port': os.getenv("MSSQL_STAGING_PORT", "1433"),
        'database': os.getenv("MSSQL_STAGING_DATABASE", "gochat247"),
        'auth_type': 'kerberos',
        'library': 'pyodbc'
    }
}

# Import the correct library based on environment
if ENVIRONMENT == 'production' or ENVIRONMENT == 'staging':
    import pyodbc as odbc
else:
    import pypyodbc as odbc

# OpenAI setup
OpenAI.api_key = os.getenv("OPENAI_API_KEY")
if not OpenAI.api_key:
    raise ValueError("OpenAI API key not found.")

client = OpenAI()

# Constants
TABLE_NAMES = [
    "CombinedDevice",
    "CombinedPlan", 
    "CombinedDigitalProduct",
    "CombinedAddon",
    "CombinedSupport",
    "CombinedRoamingPlan",
    "CombinedBlog",
    "CombinedBio",
    "CombinedReviewSentiment"
]

PRIMARY_KEY_MAP = {
    "CombinedReviewSentiment": "review_id",
    "CombinedDevice": "internal_id",
    "CombinedPlan": "internal_id",
    "CombinedDigitalProduct": "internal_id",
    "CombinedAddon": "internal_id",
    "CombinedSupport": "internal_id",
    "CombinedRoamingPlan": "internal_id",
    "CombinedBlog": "internal_id",
    "CombinedBio": "internal_id"
}

BATCH_SIZE = 10

# Set up logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# FastAPI app
app = FastAPI(
    title="Embeddings Service",
    description="Service for generating and managing text embeddings",
    version="1.0.0"
)

# Pydantic models
class EmbeddingRequest(BaseModel):
    table_names: Optional[List[str]] = None
    force_regenerate: bool = False

class EmbeddingResponse(BaseModel):
    status: str
    message: str
    processed_tables: List[str]
    errors: List[str] = []

class TableProcessingStatus(BaseModel):
    table_name: str
    status: str
    records_processed: int
    errors: List[str] = []

# Database connection functions
def get_mssql_connection_string(auth_type=None, environment=None):
    """Create MSSQL connection string for different environments and auth types"""
    if environment is None:
        environment = ENVIRONMENT
    
    config = MSSQL_CONFIG.get(environment, MSSQL_CONFIG['local'])
    
    if auth_type is None:
        auth_type = config.get('auth_type', 'windows')
    
    driver = config['driver']
    server = config['server']
    database = config['database']
    
    if auth_type.lower() == "windows":
        connection_string = f'DRIVER={{{driver}}};SERVER={server};DATABASE={database};Trusted_Connection=yes'
        return connection_string
        
    elif auth_type.lower() == "kerberos":
        port = config.get('port', '1433')
        connection_string = (
            f"DRIVER={{{driver}}};"
            f"SERVER={server},{port};"
            f"DATABASE={database};"
            "Authentication=ActiveDirectoryIntegrated;"
            "TrustServerCertificate=yes;"
        )
        return connection_string
        
    elif auth_type.lower() == "sql":
        username = os.getenv("MSSQL_USERNAME")
        password = os.getenv("MSSQL_PASSWORD")
        if not username or not password:
            raise ValueError("SQL Server authentication requires MSSQL_USERNAME and MSSQL_PASSWORD environment variables")
        
        if 'port' in config:
            server_with_port = f"{server},{config['port']}"
        else:
            server_with_port = server
            
        connection_string = f'DRIVER={{{driver}}};SERVER={server_with_port};DATABASE={database};UID={username};PWD={password}'
        return connection_string
        
    else:
        raise ValueError(f"Unsupported auth_type: {auth_type}. Use 'windows', 'kerberos', or 'sql'")

@contextmanager
def get_db_connection():
    """Context manager for database connections"""
    conn_str = get_mssql_connection_string()
    cnx = None
    cursor = None
    try:
        cnx = odbc.connect(conn_str)
        cursor = cnx.cursor()
        yield cursor, cnx
    except Exception as e:
        if cnx:
            cnx.rollback()
        logger.error(f"Database connection error: {e}")
        raise
    finally:
        if cursor:
            cursor.close()
        if cnx:
            cnx.close()

# Core embedding functions
def get_text_columns(cursor, table_name):
    """Fetch text columns from a table using INFORMATION_SCHEMA."""
    logger.debug(f"Getting text columns for table {table_name}.")
    cursor.execute("""
        SELECT COLUMN_NAME, DATA_TYPE, CHARACTER_MAXIMUM_LENGTH
        FROM INFORMATION_SCHEMA.COLUMNS 
        WHERE TABLE_NAME = ?
        AND DATA_TYPE IN ('varchar', 'nvarchar', 'text', 'ntext', 'char', 'nchar')
    """, (table_name,))
    columns = cursor.fetchall()
    text_columns = [col[0] for col in columns]
    logger.debug(f"Found text columns in {table_name}: {text_columns}")
    return text_columns

def create_embedding(text):
    """Generate an embedding for the given text using OpenAI's API."""
    logger.debug(f"Creating embedding for text (first 50 chars): {text[:50]}...")
    response = client.embeddings.create(
        input=text,
        model="text-embedding-3-small"
    )
    embedding = response.data[0].embedding
    logger.debug(f"Embedding created of length: {len(embedding)}")
    return embedding

def create_embedding_table(cursor, embedding_table):
    """Create a new table for embeddings if it doesn't exist."""
    logger.debug(f"Creating embedding table if not exists: {embedding_table}")
    create_table_query = f"""
    IF NOT EXISTS (
        SELECT * FROM INFORMATION_SCHEMA.TABLES 
        WHERE TABLE_NAME = '{embedding_table}'
    )
    BEGIN
        CREATE TABLE {embedding_table} (
            id INT IDENTITY(1,1) PRIMARY KEY,
            original_id VARCHAR(255),
            embedding NVARCHAR(MAX)
        )
    END
    """
    cursor.execute(create_table_query)

def record_embedding_exists(cursor, embedding_table, original_id):
    """Check if the embedding for the given original_id already exists."""
    check_query = f"SELECT COUNT(*) FROM {embedding_table} WHERE original_id = ?"
    cursor.execute(check_query, (original_id,))
    count = cursor.fetchone()[0]
    exists = count > 0
    logger.debug(f"Record {original_id} exists in {embedding_table}: {exists}")
    return exists

def insert_embedding_batch(cursor, cnx, embedding_table, batch_inserts):
    """Inserts a batch of embeddings into the embedding_table."""
    if batch_inserts:
        insert_query = f"INSERT INTO {embedding_table} (original_id, embedding) VALUES (?, ?)"
        try:
            cursor.executemany(insert_query, batch_inserts)
            cnx.commit()
            logger.debug(f"Inserted batch of {len(batch_inserts)} embeddings using executemany")
        except Exception as e:
            logger.warning(f"executemany failed, falling back to individual inserts: {e}")
            for insert_data in batch_inserts:
                cursor.execute(insert_query, insert_data)
            cnx.commit()
            logger.debug(f"Inserted batch of {len(batch_inserts)} embeddings using individual inserts")

def process_generic_table(cursor, cnx, table_name, force_regenerate=False):
    """Process a table by generating embeddings from concatenated text columns."""
    logger.debug(f"Starting processing for table: {table_name}")
    embedding_table = table_name + "Embedding"
    create_embedding_table(cursor, embedding_table)

    text_columns = get_text_columns(cursor, table_name)
    if not text_columns:
        logger.debug(f"No text columns found in {table_name}. Skipping.")
        return 0

    cursor.execute(f"SELECT * FROM {table_name}")
    records = cursor.fetchall()
    columns = [desc[0] for desc in cursor.description]
    primary_key_col = PRIMARY_KEY_MAP.get(table_name, columns[0])
    logger.debug(f"Using {primary_key_col} as primary key for table {table_name}")

    batch_inserts = []
    processed_count = 0

    for record in records:
        record_dict = dict(zip(columns, record))
        original_id = record_dict.get(primary_key_col)
        if original_id is None:
            logger.debug(f"Warning: primary key ({primary_key_col}) is None for record: {record_dict}")
            continue

        if not force_regenerate and record_embedding_exists(cursor, embedding_table, original_id):
            logger.debug(f"Skipping record {original_id} from {table_name}, embedding already exists.")
            continue

        # If force_regenerate, delete existing embedding first
        if force_regenerate and record_embedding_exists(cursor, embedding_table, original_id):
            delete_query = f"DELETE FROM {embedding_table} WHERE original_id = ?"
            cursor.execute(delete_query, (original_id,))

        text_content = " ".join([str(record_dict[col]) for col in text_columns if record_dict.get(col)])
        if not text_content.strip():
            logger.debug(f"Record {original_id} has no text content. Skipping.")
            continue

        try:
            embedding_vector = create_embedding(text_content)
        except Exception as e:
            logger.error(f"Error generating embedding for record {original_id} in {table_name}: {e}")
            continue

        batch_inserts.append((original_id, json.dumps(embedding_vector)))
        processed_count += 1
        
        if len(batch_inserts) >= BATCH_SIZE:
            insert_embedding_batch(cursor, cnx, embedding_table, batch_inserts)
            batch_inserts = []

    if batch_inserts:
        insert_embedding_batch(cursor, cnx, embedding_table, batch_inserts)

    logger.debug(f"Processed {processed_count} records for table {table_name}")
    return processed_count

def process_combined_devices(cursor, cnx, force_regenerate=False):
    """Custom processing for the combined_devices table."""
    table_name = "CombinedDevice"
    logger.debug(f"Starting custom processing for table: {table_name}")
    embedding_table = table_name + "Embedding"
    create_embedding_table(cursor, embedding_table)

    selected_text_columns = [
        "device_name", "description", "brand", "brand_name", "model_number",
        "memory_details", "storage_details", "dimensions_details", 
        "display_details", "connectivity", "camera", "camera_details",
        "additional_details", "additional_details_2"
    ]
    
    cursor.execute(f"SELECT * FROM {table_name}")
    records = cursor.fetchall()
    columns = [desc[0] for desc in cursor.description]
    primary_key_col = PRIMARY_KEY_MAP.get(table_name, columns[0])

    batch_inserts = []
    processed_count = 0

    for record in records:
        record_dict = dict(zip(columns, record))
        original_id = record_dict.get(primary_key_col)
        if original_id is None:
            continue

        if not force_regenerate and record_embedding_exists(cursor, embedding_table, original_id):
            continue

        if force_regenerate and record_embedding_exists(cursor, embedding_table, original_id):
            delete_query = f"DELETE FROM {embedding_table} WHERE original_id = ?"
            cursor.execute(delete_query, (original_id,))

        text_parts = [str(record_dict[col]) for col in selected_text_columns if record_dict.get(col)]
        text_content = " ".join(text_parts)
        if not text_content.strip():
            continue

        try:
            embedding_vector = create_embedding(text_content)
        except Exception as e:
            logger.error(f"Error generating embedding for record {original_id} in {table_name}: {e}")
            continue

        batch_inserts.append((original_id, json.dumps(embedding_vector)))
        processed_count += 1
        
        if len(batch_inserts) >= BATCH_SIZE:
            insert_embedding_batch(cursor, cnx, embedding_table, batch_inserts)
            batch_inserts = []

    if batch_inserts:
        insert_embedding_batch(cursor, cnx, embedding_table, batch_inserts)

    logger.debug(f"Processed {processed_count} records for table {table_name}")
    return processed_count

# Map each table to its specific processing function
TABLE_PROCESSORS = {
    "CombinedDevice": process_combined_devices,
    "CombinedReviewSentiment": process_generic_table
}

# FastAPI Routes
@app.get("/")
async def root():
    return {"message": "Embeddings Service API", "version": "1.0.0"}

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    try:
        with get_db_connection() as (cursor, cnx):
            cursor.execute("SELECT 1")
            cursor.fetchone()
        return {"status": "healthy", "database": "connected"}
    except Exception as e:
        return JSONResponse(
            status_code=503,
            content={"status": "unhealthy", "database": "disconnected", "error": str(e)}
        )

@app.get("/tables")
async def get_available_tables():
    """Get list of available tables for embedding processing"""
    return {"tables": TABLE_NAMES}

@app.post("/embeddings/generate", response_model=EmbeddingResponse)
async def generate_embeddings(request: EmbeddingRequest, background_tasks: BackgroundTasks):
    """Generate embeddings for specified tables"""
    try:
        tables_to_process = request.table_names if request.table_names else TABLE_NAMES
        
        # Validate table names
        invalid_tables = [t for t in tables_to_process if t not in TABLE_NAMES]
        if invalid_tables:
            raise HTTPException(
                status_code=400, 
                detail=f"Invalid table names: {invalid_tables}. Available tables: {TABLE_NAMES}"
            )
        
        # Process tables in background
        background_tasks.add_task(
            process_tables_background, 
            tables_to_process, 
            request.force_regenerate
        )
        
        return EmbeddingResponse(
            status="accepted",
            message="Embedding generation started in background",
            processed_tables=tables_to_process
        )
        
    except Exception as e:
        logger.error(f"Error in generate_embeddings: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/embeddings/generate-sync", response_model=EmbeddingResponse)
async def generate_embeddings_sync(request: EmbeddingRequest):
    """Generate embeddings synchronously (blocks until complete)"""
    try:
        tables_to_process = request.table_names if request.table_names else TABLE_NAMES
        
        # Validate table names
        invalid_tables = [t for t in tables_to_process if t not in TABLE_NAMES]
        if invalid_tables:
            raise HTTPException(
                status_code=400, 
                detail=f"Invalid table names: {invalid_tables}. Available tables: {TABLE_NAMES}"
            )
        
        processed_tables = []
        errors = []
        
        with get_db_connection() as (cursor, cnx):
            for table in tables_to_process:
                try:
                    processor = TABLE_PROCESSORS.get(table, process_generic_table)
                    if processor == process_generic_table:
                        processed_count = processor(cursor, cnx, table, request.force_regenerate)
                    else:
                        processed_count = processor(cursor, cnx, request.force_regenerate)
                    
                    processed_tables.append(table)
                    logger.info(f"Successfully processed {processed_count} records for table {table}")
                    
                except Exception as e:
                    error_msg = f"Error processing table {table}: {str(e)}"
                    logger.error(error_msg)
                    errors.append(error_msg)
        
        status = "completed" if not errors else "completed_with_errors"
        message = f"Processed {len(processed_tables)} tables successfully"
        if errors:
            message += f" with {len(errors)} errors"
            
        return EmbeddingResponse(
            status=status,
            message=message,
            processed_tables=processed_tables,
            errors=errors
        )
        
    except Exception as e:
        logger.error(f"Error in generate_embeddings_sync: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/embeddings/status/{table_name}")
async def get_embedding_status(table_name: str):
    """Get embedding status for a specific table"""
    if table_name not in TABLE_NAMES:
        raise HTTPException(
            status_code=404, 
            detail=f"Table {table_name} not found. Available tables: {TABLE_NAMES}"
        )
    
    try:
        with get_db_connection() as (cursor, cnx):
            embedding_table = table_name + "Embedding"
            
            # Check if embedding table exists
            cursor.execute("""
                SELECT COUNT(*) FROM INFORMATION_SCHEMA.TABLES 
                WHERE TABLE_NAME = ?
            """, (embedding_table,))
            
            if cursor.fetchone()[0] == 0:
                return {
                    "table_name": table_name,
                    "embedding_table_exists": False,
                    "total_embeddings": 0,
                    "original_records": 0
                }
            
            # Count embeddings
            cursor.execute(f"SELECT COUNT(*) FROM {embedding_table}")
            embedding_count = cursor.fetchone()[0]
            
            # Count original records
            cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
            original_count = cursor.fetchone()[0]
            
            return {
                "table_name": table_name,
                "embedding_table_exists": True,
                "total_embeddings": embedding_count,
                "original_records": original_count,
                "completion_percentage": round((embedding_count / original_count * 100), 2) if original_count > 0 else 0
            }
            
    except Exception as e:
        logger.error(f"Error getting status for table {table_name}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def process_tables_background(tables: List[str], force_regenerate: bool = False):
    """Background task to process tables"""
    try:
        with get_db_connection() as (cursor, cnx):
            for table in tables:
                try:
                    processor = TABLE_PROCESSORS.get(table, process_generic_table)
                    if processor == process_generic_table:
                        processed_count = processor(cursor, cnx, table, force_regenerate)
                    else:
                        processed_count = processor(cursor, cnx, force_regenerate)
                    
                    logger.info(f"Background task: Successfully processed {processed_count} records for table {table}")
                    
                except Exception as e:
                    logger.error(f"Background task: Error processing table {table}: {e}")
                    
    except Exception as e:
        logger.error(f"Background task: Database connection error: {e}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
